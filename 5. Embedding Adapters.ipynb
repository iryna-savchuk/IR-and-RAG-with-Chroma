{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f057401b-a4ee-462a-9ba9-3e342e06eb1a",
   "metadata": {},
   "source": [
    "# Embedding Adapters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb907cb7",
   "metadata": {},
   "source": [
    "Embedding adapters are a way to alter the embedding of a query directly in order to produce better retrieval results. \n",
    "\n",
    "In effect, we insert an additional stage in the retrieval system called an embedding adapter, which happens after the embedding model, but before we retrieve the most relevant results. We train the embedding adapter using user feedback on the relevancy of our retrieved results for a set of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65337e9-85ee-47f7-89fd-7fe77cd0e1b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from helper_utils import load_chroma, word_wrap, project_embeddings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "import numpy as np\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7794092-4195-4cf3-9eab-11c9c05a26b9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "chroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', \n",
    "                                collection_name='microsoft_annual_report_2022', \n",
    "                                embedding_function=embedding_function)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26a01a-4575-446b-b8dc-a8c5ab153172",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n",
    "projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ff293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db77c598",
   "metadata": {},
   "source": [
    "## Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a dataset using LM model\n",
    "# (ideally, data should be created by users utilising our RAG app)\n",
    "def generate_queries(model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert financial research assistant. You help users analyze financial statements to better understand companies. \"\n",
    "            \"Suggest 10 to 15 short questions that are important to ask when analyzing an annual report. \"\n",
    "            \"Do not output any compound questions (questions with multiple sentences or conjunctions).\"\n",
    "            \"Output each question on a separate line divided by a newline.\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    content = content.split(\"\\n\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0131b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_queries = generate_queries()\n",
    "for query in generated_queries:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_collection.query(query_texts=generated_queries, n_results=10, include=['documents', 'embeddings'])\n",
    "retrieved_documents = results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionn to Generate Labels \n",
    "def evaluate_results(query, statement, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful expert financial research assistant. You help users analyze financial statements to better understand companies. \"\n",
    "        \"For the given query, evaluate whether the following satement is relevant.\"\n",
    "        \"Output only 'yes' or 'no'.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Query: {query}, Statement: {statement}\"\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=1\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    if content == \"yes\":\n",
    "        return 1\n",
    "    return -1  # We are going to use cosine similarity \n",
    "               # 1: vectors are pointed in the same direction, \n",
    "               # -1: vectors are pointed in the oposite directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_embeddings = results['embeddings']\n",
    "query_embeddings = embedding_function(generated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_query_embeddings = []\n",
    "adapter_doc_embeddings = []\n",
    "adapter_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa06c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, query in enumerate(tqdm(generated_queries)):\n",
    "    for d, document in enumerate(retrieved_documents[q]):\n",
    "        adapter_query_embeddings.append(query_embeddings[q])\n",
    "        adapter_doc_embeddings.append(retrieved_embeddings[q][d])\n",
    "        adapter_labels.append(evaluate_results(query, document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adapter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data to torch tensors\n",
    "adapter_query_embeddings = torch.Tensor(np.array(adapter_query_embeddings))\n",
    "adapter_doc_embeddings = torch.Tensor(np.array(adapter_doc_embeddings))\n",
    "adapter_labels = torch.Tensor(np.expand_dims(np.array(adapter_labels),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Torch Tensor Dataset that will be used for the model training\n",
    "dataset = torch.utils.data.TensorDataset(adapter_query_embeddings, \n",
    "                                        adapter_doc_embeddings, \n",
    "                                        adapter_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3c6f455",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e41e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(query_embedding, document_embedding, adaptor_matrix):\n",
    "    updated_query_embedding = torch.matmul(adaptor_matrix, query_embedding)\n",
    "    return torch.cosine_similarity(updated_query_embedding, document_embedding, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(query_embedding, document_embedding, adaptor_matrix, label):\n",
    "    return torch.nn.MSELoss()(model(query_embedding, document_embedding, adaptor_matrix), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d25776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the adaptor matrix\n",
    "mat_size = len(adapter_query_embeddings[0])\n",
    "adapter_matrix = torch.randn(mat_size, mat_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74eb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = float('inf')\n",
    "best_matrix = None\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for query_embedding, document_embedding, label in dataset:\n",
    "        loss = mse_loss(query_embedding, document_embedding, adapter_matrix, label)\n",
    "\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            best_matrix = adapter_matrix.clone().detach().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            adapter_matrix -= 0.01 * adapter_matrix.grad\n",
    "            adapter_matrix.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best loss: {min_loss.detach().numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9697686e",
   "metadata": {},
   "source": [
    "Let's take a look at as how the adapter matrix influences our query vector. \n",
    "\n",
    "To do that, we can construct a test vector consisting of all ones, and we can multiply that test vector by our best matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = torch.ones((mat_size,1))  # a test vector consisting of ones\n",
    "scaled_vector = np.matmul(best_matrix, test_vector).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing which dimensions of our vectors get scaled by what amount\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(scaled_vector)), scaled_vector.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175122d3",
   "metadata": {},
   "source": [
    "**Note**: An embedding adapter can be seen as stretching space for the dimensions which are most relevant to the particular queries that we have and and squeezing space for the dimensions that are not relevant to our query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = embedding_function(generated_queries)\n",
    "adapted_query_embeddings = np.matmul(best_matrix, np.array(query_embeddings).T).T\n",
    "\n",
    "projected_query_embeddings = project_embeddings(query_embeddings, umap_transform)\n",
    "projected_adapted_query_embeddings = project_embeddings(adapted_query_embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9769670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the projected query and retrieved documents in the embedding space\n",
    "plt.figure()\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
    "plt.scatter(projected_query_embeddings[:, 0], projected_query_embeddings[:, 1], s=150, marker='X', color='r', label=\"original\")\n",
    "plt.scatter(projected_adapted_query_embeddings[:, 0], projected_adapted_query_embeddings[:, 1], s=150, marker='X', color='green', label=\"adapted\")\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title(\"Adapted Queries\")\n",
    "plt.axis('off')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
